{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d0ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_cognite_transformation_files(base_path):\n",
    "    \"\"\"\n",
    "    Guides the user to create the necessary files for Cognite Transformations\n",
    "    based on the provided instructions.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base file path where the transformations directory\n",
    "                         structure will be created (e.g., /Users/yourname/projects/ice-cream-dataops).\n",
    "    \"\"\"\n",
    "    # Validate base path\n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Error: The provided base path '{base_path}' does not exist.\")\n",
    "        return\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Error: The provided base path '{base_path}' is not a directory.\")\n",
    "        return\n",
    "\n",
    "    transformations_dir_relative = os.path.join('modules', 'bootcamp', 'ice_cream_api', 'transformations')\n",
    "    full_transformations_path = os.path.join(base_path, transformations_dir_relative)\n",
    "\n",
    "    print(f\"\\nAttempting to create directory: {full_transformations_path}\")\n",
    "    try:\n",
    "        os.makedirs(full_transformations_path, exist_ok=True)\n",
    "        print(f\"Directory '{full_transformations_path}' ensured to exist.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory '{full_transformations_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Define content for create_asset_hierarchy transformation ---\n",
    "    create_asset_hierarchy_yaml_content = \"\"\"externalId: create_asset_hierarchy\n",
    "name: Create Cognite Asset Hierarchy\n",
    "destination:\n",
    "  dataModel:\n",
    "    space: cdf_cdm\n",
    "    externalId: CogniteCore\n",
    "    version: v1\n",
    "    destinationType: CogniteAsset\n",
    "  instanceSpace: icapi_dm_space\n",
    "  type: instances\n",
    "ignoreNullFields: true\n",
    "isPublic: true\n",
    "conflictMode: upsert\n",
    "authentication:\n",
    "  clientId: {{ icapi_extractors_client_id }}\n",
    "  clientSecret: {{ icapi_extractors_client_secret }}\n",
    "  tokenUri: {{ tokenUri }}\n",
    "  cdfProjectName: {{ cdfProjectName }}\n",
    "  scopes: {{ scopes }}\n",
    "\"\"\"\n",
    "\n",
    "    create_asset_hierarchy_sql_content = \"\"\"-- SQL query for create_asset_hierarchy.Transformation.sql\n",
    "-- (Replace this with your actual SQL query from Fusion UI)\n",
    "-- Example placeholder SQL:\n",
    "SELECT\n",
    "    'asset_root' AS externalId,\n",
    "    NULL AS parentExternalId,\n",
    "    'Root Asset' AS name,\n",
    "    'root' AS type\n",
    "UNION ALL\n",
    "SELECT\n",
    "    'ice_cream_factory_1' AS externalId,\n",
    "    'asset_root' AS parentExternalId,\n",
    "    'Ice Cream Factory 1' AS name,\n",
    "    'factory' AS type\n",
    "UNION ALL\n",
    "SELECT\n",
    "    'machine_a_factory_1' AS externalId,\n",
    "    'ice_cream_factory_1' AS parentExternalId,\n",
    "    'Machine A' AS name,\n",
    "    'machine' AS type;\n",
    "\"\"\"\n",
    "\n",
    "    create_asset_hierarchy_schedule_content = \"\"\"\n",
    "# We recommend running the schedule every night at the 0 hour and the minute should be your project number.\n",
    "# Example: If your project number is 42, set minute: 42\n",
    "name: create_asset_hierarchy_schedule\n",
    "externalId: create_asset_hierarchy_schedule\n",
    "transformationExternalId: create_asset_hierarchy\n",
    "schedule:\n",
    "  interval:\n",
    "    unit: days\n",
    "    value: 1\n",
    "  time:\n",
    "    hour: 0\n",
    "    minute: 0 # IMPORTANT: Change this to your project number\n",
    "\"\"\"\n",
    "\n",
    "    # --- Define content for contextualize_ts_assets transformation ---\n",
    "    contextualize_ts_assets_yaml_content = \"\"\"externalId: contextualize_ts_assets\n",
    "name: Contextualize TimeSeries and Assets\n",
    "destination:\n",
    "  dataModel:\n",
    "    space: cdf_cdm\n",
    "    externalId: CogniteCore\n",
    "    version: v1\n",
    "    destinationType: CogniteTimeSeries\n",
    "  type: instances\n",
    "ignoreNullFields: true\n",
    "conflictMode: upsert\n",
    "authentication:\n",
    "  clientId: {{ icapi_extractors_client_id }}\n",
    "  clientSecret: {{ icapi_extractors_client_secret }}\n",
    "  tokenUri: {{ tokenUri }}\n",
    "  cdfProjectName: {{ cdfProjectName }}\n",
    "  scopes: {{ scopes }}\n",
    "\"\"\"\n",
    "\n",
    "    contextualize_ts_assets_sql_content = \"\"\"select\n",
    "  timeseries.externalId,\n",
    "  array(node_reference(\"icapi_dm_space\", assets.externalId)) as assets,\n",
    "  timeseries.isStep,\n",
    "  timeseries.type,\n",
    "  timeseries.space\n",
    "from\n",
    "  cdf_data_models(\"cdf_cdm\", \"CogniteCore\", \"v1\", \"CogniteTimeSeries\") as timeseries\n",
    "left join\n",
    "  cdf_data_models(\"cdf_cdm\", \"CogniteCore\", \"v1\", \"CogniteAsset\") as assets\n",
    "ON\n",
    "  split_part(timeseries.externalId, \":\", 1) = assets.externalId\n",
    "\"\"\"\n",
    "\n",
    "    files_to_create = {\n",
    "        \"create_asset_hierarchy.Transformation.yaml\": create_asset_hierarchy_yaml_content,\n",
    "        \"create_asset_hierarchy.Transformation.sql\": create_asset_hierarchy_sql_content,\n",
    "        \"create_asset_hierarchy.schedule.yaml\": create_asset_hierarchy_schedule_content,\n",
    "        \"contextualize_ts_assets.Transformation.yaml\": contextualize_ts_assets_yaml_content,\n",
    "        \"contextualize_ts_assets.Transformation.sql\": contextualize_ts_assets_sql_content,\n",
    "    }\n",
    "\n",
    "    for filename, content in files_to_create.items():\n",
    "        file_path = os.path.join(full_transformations_path, filename)\n",
    "        print(f\"Creating file: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Successfully created '{filename}'.\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error writing to file '{file_path}': {e}\")\n",
    "\n",
    "    print(\"\\nAll specified files and directories have been processed.\")\n",
    "    print(\"Remember to replace the placeholder SQL query in 'create_asset_hierarchy.Transformation.sql' with your actual query.\")\n",
    "    print(\"Also, adjust the 'minute' in 'create_asset_hierarchy.schedule.yaml' to your project number.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7fd7d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to create directory: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations\n",
      "Directory 'C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations' ensured to exist.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations\\create_asset_hierarchy.Transformation.yaml\n",
      "Successfully created 'create_asset_hierarchy.Transformation.yaml'.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations\\create_asset_hierarchy.Transformation.sql\n",
      "Successfully created 'create_asset_hierarchy.Transformation.sql'.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations\\create_asset_hierarchy.schedule.yaml\n",
      "Successfully created 'create_asset_hierarchy.schedule.yaml'.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations\\contextualize_ts_assets.Transformation.yaml\n",
      "Successfully created 'contextualize_ts_assets.Transformation.yaml'.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\transformations\\contextualize_ts_assets.Transformation.sql\n",
      "Successfully created 'contextualize_ts_assets.Transformation.sql'.\n",
      "\n",
      "All specified files and directories have been processed.\n",
      "Remember to replace the placeholder SQL query in 'create_asset_hierarchy.Transformation.sql' with your actual query.\n",
      "Also, adjust the 'minute' in 'create_asset_hierarchy.schedule.yaml' to your project number.\n"
     ]
    }
   ],
   "source": [
    "# --- How to use this in a Jupyter Notebook ---\n",
    "# Define your base path here:\n",
    "my_base_path = r\"C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\" # <--- IMPORTANT: Change this to your actual path\n",
    "\n",
    "# Call the function with your base path\n",
    "create_cognite_transformation_files(my_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9faf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_cognite_toolkit_files(base_path):\n",
    "    \"\"\"\n",
    "    Guides the user to create the necessary files for Cognite Transformations and Workflows\n",
    "    based on the provided instructions.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base file path where the directory structure will be created\n",
    "                         (e.g., /Users/yourname/projects/ice-cream-dataops).\n",
    "    \"\"\"\n",
    "    # Validate base path\n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Error: The provided base path '{base_path}' does not exist.\")\n",
    "        return\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Error: The provided base path '{base_path}' is not a directory.\")\n",
    "        return\n",
    "\n",
    "    # --- Define paths ---\n",
    "    ice_cream_api_dir_relative = os.path.join('modules', 'bootcamp', 'ice_cream_api')\n",
    "    full_ice_cream_api_path = os.path.join(base_path, ice_cream_api_dir_relative)\n",
    "\n",
    "    transformations_dir = os.path.join(full_ice_cream_api_path, 'transformations')\n",
    "    workflows_dir = os.path.join(full_ice_cream_api_path, 'workflows')\n",
    "\n",
    "    # --- Ensure directories exist ---\n",
    "    for directory in [transformations_dir, workflows_dir]:\n",
    "        print(f\"\\nAttempting to create directory: {directory}\")\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            print(f\"Directory '{directory}' ensured to exist.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory '{directory}': {e}\")\n",
    "            return\n",
    "\n",
    "    # --- Content for Transformations ---\n",
    "    create_asset_hierarchy_yaml_content = \"\"\"externalId: create_asset_hierarchy\n",
    "name: Create Cognite Asset Hierarchy\n",
    "destination:\n",
    "  dataModel:\n",
    "    space: cdf_cdm\n",
    "    externalId: CogniteCore\n",
    "    version: v1\n",
    "    destinationType: CogniteAsset\n",
    "  instanceSpace: icapi_dm_space\n",
    "  type: instances\n",
    "ignoreNullFields: true\n",
    "isPublic: true\n",
    "conflictMode: upsert\n",
    "authentication:\n",
    "  clientId: {{ icapi_extractors_client_id }}\n",
    "  clientSecret: {{ icapi_extractors_client_secret }}\n",
    "  tokenUri: {{ tokenUri }}\n",
    "  cdfProjectName: {{ cdfProjectName }}\n",
    "  scopes: {{ scopes }}\n",
    "\"\"\"\n",
    "\n",
    "    create_asset_hierarchy_sql_content = \"\"\"-- SQL query for create_asset_hierarchy.Transformation.sql\n",
    "-- IMPORTANT: Replace this with your actual SQL query from Fusion UI or your source.\n",
    "-- Example placeholder SQL:\n",
    "SELECT\n",
    "    'asset_root' AS externalId,\n",
    "    NULL AS parentExternalId,\n",
    "    'Root Asset' AS name,\n",
    "    'root' AS type\n",
    "UNION ALL\n",
    "SELECT\n",
    "    'ice_cream_factory_1' AS externalId,\n",
    "    'asset_root' AS parentExternalId,\n",
    "    'Ice Cream Factory 1' AS name,\n",
    "    'factory' AS type\n",
    "UNION ALL\n",
    "SELECT\n",
    "    'machine_a_factory_1' AS externalId,\n",
    "    'ice_cream_factory_1' AS parentExternalId,\n",
    "    'Machine A' AS name,\n",
    "    'machine' AS type;\n",
    "\"\"\"\n",
    "\n",
    "    create_asset_hierarchy_schedule_content = \"\"\"\n",
    "# We recommend running the schedule every night at the 0 hour and the minute should be your project number.\n",
    "# Example: If your project number is 42, set minute: 42\n",
    "name: create_asset_hierarchy_schedule\n",
    "externalId: create_asset_hierarchy_schedule\n",
    "transformationExternalId: create_asset_hierarchy\n",
    "schedule:\n",
    "  interval:\n",
    "    unit: days\n",
    "    value: 1\n",
    "  time:\n",
    "    hour: 0\n",
    "    minute: 0 # IMPORTANT: Change this to your project number\n",
    "\"\"\"\n",
    "\n",
    "    contextualize_ts_assets_yaml_content = \"\"\"externalId: contextualize_ts_assets\n",
    "name: Contextualize TimeSeries and Assets\n",
    "destination:\n",
    "  dataModel:\n",
    "    space: cdf_cdm\n",
    "    externalId: CogniteCore\n",
    "    version: v1\n",
    "    destinationType: CogniteTimeSeries\n",
    "  type: instances\n",
    "ignoreNullFields: true\n",
    "conflictMode: upsert\n",
    "authentication:\n",
    "  clientId: {{ icapi_extractors_client_id }}\n",
    "  clientSecret: {{ icapi_extractors_client_secret }}\n",
    "  tokenUri: {{ tokenUri }}\n",
    "  cdfProjectName: {{ cdfProjectName }}\n",
    "  scopes: {{ scopes }}\n",
    "\"\"\"\n",
    "\n",
    "    contextualize_ts_assets_sql_content = \"\"\"select\n",
    "  timeseries.externalId,\n",
    "  array(node_reference(\"icapi_dm_space\", assets.externalId)) as assets,\n",
    "  timeseries.isStep,\n",
    "  timeseries.type,\n",
    "  timeseries.space\n",
    "from\n",
    "  cdf_data_models(\"cdf_cdm\", \"CogniteCore\", \"v1\", \"CogniteTimeSeries\") as timeseries\n",
    "left join\n",
    "  cdf_data_models(\"cdf_cdm\", \"CogniteCore\", \"v1\", \"CogniteAsset\") as assets\n",
    "ON\n",
    "  split_part(timeseries.externalId, \":\", 1) = assets.externalId\n",
    "\"\"\"\n",
    "\n",
    "    # --- Content for Workflows ---\n",
    "    wf_icapi_data_pipeline_workflow_content = \"\"\"externalId: wf_icapi_data_pipeline\n",
    "description: A workflow to orchestrate the ice cream API data pipeline, including asset hierarchy creation, time series contextualization, data extraction, and OEE calculation.\n",
    "\"\"\"\n",
    "\n",
    "    wf_icapi_data_pipeline_workflow_version_content = \"\"\"workflowExternalId: wf_icapi_data_pipeline\n",
    "version: '1'\n",
    "workflowDefinition:\n",
    "  tasks:\n",
    "  - externalId: create_asset_hierarchy_task\n",
    "    type: transformation\n",
    "    parameters:\n",
    "      transformation:\n",
    "        externalId: create_asset_hierarchy\n",
    "    retries: 3\n",
    "    timeout: 3600\n",
    "    onFailure: abortWorkflow\n",
    "  - externalId: contextualize_ts_assets_task\n",
    "    type: transformation\n",
    "    parameters:\n",
    "      transformation:\n",
    "        externalId: contextualize_ts_assets\n",
    "    retries: 3\n",
    "    timeout: 3600\n",
    "    onFailure: abortWorkflow\n",
    "    dependsOn:\n",
    "      - externalId: create_asset_hierarchy_task\n",
    "  - externalId: icapi_datapoints_extractor_task\n",
    "    type: function\n",
    "    parameters:\n",
    "      function:\n",
    "        externalId: icapi_datapoints_extractor\n",
    "        data: { \"hours\": 1 } # As per example in instructions\n",
    "    retries: 3\n",
    "    timeout: 3600\n",
    "    onFailure: abortWorkflow\n",
    "    dependsOn:\n",
    "      - externalId: contextualize_ts_assets_task\n",
    "  - externalId: oee_timeseries_task\n",
    "    type: function\n",
    "    parameters:\n",
    "      function:\n",
    "        externalId: oee_timeseries\n",
    "        data: { \"start_time\": \"24h-ago\" } # Placeholder data, adjust as needed for oee_timeseries function\n",
    "    retries: 3\n",
    "    timeout: 3600\n",
    "    onFailure: abortWorkflow\n",
    "    dependsOn:\n",
    "      - externalId: icapi_datapoints_extractor_task\n",
    "\"\"\"\n",
    "\n",
    "    wf_icapi_data_pipeline_workflow_trigger_content = \"\"\"externalId: icapi_trigger\n",
    "triggerRule:\n",
    "  triggerType: schedule\n",
    "  cronExpression: '*/15 * * * *' # Runs every 15 minutes\n",
    "workflowExternalId: wf_icapi_data_pipeline\n",
    "workflowVersion: '1'\n",
    "authentication:\n",
    "  clientId: {{ icapi_trigger_client_id }}\n",
    "  clientSecret: {{ icapi_trigger_client_secret }}\n",
    "\"\"\"\n",
    "\n",
    "    # --- Files to create mapping ---\n",
    "    files_to_create = {\n",
    "        os.path.join(transformations_dir, \"create_asset_hierarchy.Transformation.yaml\"): create_asset_hierarchy_yaml_content,\n",
    "        os.path.join(transformations_dir, \"create_asset_hierarchy.Transformation.sql\"): create_asset_hierarchy_sql_content,\n",
    "        os.path.join(transformations_dir, \"create_asset_hierarchy.schedule.yaml\"): create_asset_hierarchy_schedule_content,\n",
    "        os.path.join(transformations_dir, \"contextualize_ts_assets.Transformation.yaml\"): contextualize_ts_assets_yaml_content,\n",
    "        os.path.join(transformations_dir, \"contextualize_ts_assets.Transformation.sql\"): contextualize_ts_assets_sql_content,\n",
    "        os.path.join(workflows_dir, \"wf_icapi_data_pipeline.WorkFlow.yaml\"): wf_icapi_data_pipeline_workflow_content,\n",
    "        os.path.join(workflows_dir, \"wf_icapi_data_pipeline.WorkflowVersion.yaml\"): wf_icapi_data_pipeline_workflow_version_content,\n",
    "        os.path.join(workflows_dir, \"wf_icapi_data_pipeline.WorkflowTrigger.yaml\"): wf_icapi_data_pipeline_workflow_trigger_content,\n",
    "    }\n",
    "\n",
    "    for file_path, content in files_to_create.items():\n",
    "        print(f\"Creating file: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Successfully created '{os.path.basename(file_path)}'.\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error writing to file '{file_path}': {e}\")\n",
    "\n",
    "    print(\"\\nAll specified files and directories have been processed.\")\n",
    "    print(\"\\n--- Important Post-Processing Steps ---\")\n",
    "    print(\"1. Replace the placeholder SQL query in 'create_asset_hierarchy.Transformation.sql' with your actual query.\")\n",
    "    print(\"2. Adjust the 'minute' in 'create_asset_hierarchy.schedule.yaml' to your project number for scheduling.\")\n",
    "    print(\"3. Review the 'data' parameters for 'icapi_datapoints_extractor_task' and 'oee_timeseries_task' in 'wf_icapi_data_pipeline.WorkflowVersion.yaml' and adjust them as per your function requirements.\")\n",
    "    print(\"4. Ensure the authentication client IDs and secrets in all YAML files (especially workflow trigger) are correctly configured in your environment.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to use this in a Jupyter Notebook ---\n",
    "# Define your base path here:\n",
    "#my_base_path = \"/Users/yourname/projects/ice-cream-dataops\" # <--- IMPORTANT: Change this to your actual path\n",
    "\n",
    "# Call the function with your base path\n",
    "create_cognite_toolkit_files(my_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ce9c7",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a26a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def explain_cognite_function_structure_and_local_testing(base_path):\n",
    "    \"\"\"\n",
    "    Explains the structure of a Cognite Function (handler.py) and how to test it locally\n",
    "    using the CDF Toolkit.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base file path (e.g., C:\\\\Users\\\\Prafull Ghare\\\\...\\\\ice-cream-dataops).\n",
    "                         This is used for context to show the expected location of handler.py.\n",
    "                         No files are created or modified by this function.\n",
    "    \"\"\"\n",
    "    print(\"--- Understanding Cognite Functions and Local Testing ---\")\n",
    "\n",
    "    # Construct the expected path for the function's handler.py\n",
    "    function_dir = os.path.join(\n",
    "        base_path,\n",
    "        'modules',\n",
    "        'bootcamp',\n",
    "        'ice_cream_api',\n",
    "        'functions',\n",
    "        'icapi_datapoints_extractor'\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBased on previous steps, your 'icapi_datapoints_extractor' function directory is expected to be at:\")\n",
    "    print(f\"'{function_dir}'\")\n",
    "\n",
    "    # Check if the directory exists (from previous script's execution)\n",
    "    if not os.path.isdir(function_dir):\n",
    "        print(f\"\\nWARNING: The directory '{function_dir}' was not found.\")\n",
    "        print(\"Please ensure the previous script that created the function directories ran successfully.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "# --- How to use this in a Jupyter Notebook ---\n",
    "# IMPORTANT: Replace this with the actual base path of your 'ice-cream-dataops' project.\n",
    "# This path should be the same as what you used for previous script executions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5295998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Understanding Cognite Functions and Local Testing ---\n",
      "\n",
      "Based on previous steps, your 'icapi_datapoints_extractor' function directory is expected to be at:\n",
      "'C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\ice_cream_api\\functions\\icapi_datapoints_extractor'\n"
     ]
    }
   ],
   "source": [
    "my_base_path = r\"C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\"\n",
    "\n",
    "# Call the function to get the explanation and instructions\n",
    "explain_cognite_function_structure_and_local_testing(my_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a213b",
   "metadata": {},
   "source": [
    "#### OEE Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4845453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_oee_function_files(base_path):\n",
    "    \"\"\"\n",
    "    Creates the necessary directory structure and YAML configuration files\n",
    "    for the OEE Cognite Function based on the provided instructions.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base file path (e.g., C:\\\\Users\\\\Prafull Ghare\\\\...\\\\ice-cream-dataops).\n",
    "    \"\"\"\n",
    "    print(\"--- Creating OEE Function Files ---\")\n",
    "\n",
    "    # Define the base path for the OEE use case\n",
    "    oee_base_dir_relative = os.path.join('modules', 'bootcamp', 'use_cases', 'oee')\n",
    "    full_oee_base_path = os.path.join(base_path, oee_base_dir_relative)\n",
    "\n",
    "    # Define paths for subdirectories\n",
    "    oee_functions_dir = os.path.join(full_oee_base_path, 'functions')\n",
    "    oee_timeseries_dir = os.path.join(oee_functions_dir, 'oee_timeseries') # This is where handler.py will go\n",
    "\n",
    "    # Ensure all necessary directories exist\n",
    "    directories_to_create = [\n",
    "        os.path.join(full_oee_base_path, 'auth'),\n",
    "        os.path.join(full_oee_base_path, 'data_sets'),\n",
    "        oee_functions_dir,\n",
    "        oee_timeseries_dir\n",
    "    ]\n",
    "\n",
    "    for directory in directories_to_create:\n",
    "        print(f\"\\nAttempting to create directory: {directory}\")\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            print(f\"Directory '{directory}' ensured to exist.\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory '{directory}': {e}\")\n",
    "            return\n",
    "\n",
    "    # --- Content for functions.Function.yaml ---\n",
    "    functions_function_yaml_content = \"\"\"- name: OEE TimeSeries\n",
    "  externalId: oee_timeseries\n",
    "  owner: CDF Bootcamp Team\n",
    "  description: Function to calculate OEE\n",
    "  metadata:\n",
    "      version: \"1.0\"\n",
    "  runtime: py311\n",
    "  functionPath: ./handler.py\n",
    "\"\"\"\n",
    "\n",
    "    # --- Content for schedules.Schedule.yaml ---\n",
    "    schedules_schedule_yaml_content = \"\"\"- name: Run calculations every 5 minutes for last hour of data\n",
    "  cronExpression: \"5-59/5 * * * *\"\n",
    "  functionExternalId: oee_timeseries\n",
    "  authentication:\n",
    "      clientId: {{ data_pipeline_oee_client_id }}\n",
    "      clientSecret: {{ data_pipeline_oee_client_secret }}\n",
    "\"\"\"\n",
    "\n",
    "    # --- Files to create mapping ---\n",
    "    files_to_create = {\n",
    "        os.path.join(oee_functions_dir, \"functions.Function.yaml\"): functions_function_yaml_content,\n",
    "        os.path.join(oee_functions_dir, \"schedules.Schedule.yaml\"): schedules_schedule_yaml_content,\n",
    "    }\n",
    "\n",
    "    for file_path, content in files_to_create.items():\n",
    "        print(f\"Creating file: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Successfully created '{os.path.basename(file_path)}'.\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error writing to file '{file_path}': {e}\")\n",
    "\n",
    "    print(\"\\nAll specified files and directories for OEE function have been processed.\")\n",
    "    print(\"\\n--- Important Post-Processing Steps for OEE Function ---\")\n",
    "    print(\"1. Remember that the 'handler.py' file for 'oee_timeseries' function is expected to be in the 'oee_timeseries' directory (which has been created), but its content is not generated by this script.\")\n",
    "    print(\"2. Ensure the authentication client IDs and secrets (e.g., `data_pipeline_oee_client_id`, `data_pipeline_oee_client_secret`) are correctly configured as environment variables in your deployment environment.\")\n",
    "    print(\"3. If you need to pass specific 'sites' or 'lookback_minutes' to the OEE function beyond its defaults, you would add a 'data' section to the schedule in 'schedules.Schedule.yaml' (e.g., `data: { sites: [\\\"site1\\\"], lookback_minutes: 120 }`).\")\n",
    "\n",
    "\n",
    "# --- How to use this in a Jupyter Notebook ---\n",
    "# IMPORTANT: Replace this with the actual base path of your 'ice-cream-dataops' project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3cf2173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating OEE Function Files ---\n",
      "\n",
      "Attempting to create directory: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\auth\n",
      "Directory 'C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\auth' ensured to exist.\n",
      "\n",
      "Attempting to create directory: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\data_sets\n",
      "Directory 'C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\data_sets' ensured to exist.\n",
      "\n",
      "Attempting to create directory: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\functions\n",
      "Directory 'C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\functions' ensured to exist.\n",
      "\n",
      "Attempting to create directory: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\functions\\oee_timeseries\n",
      "Directory 'C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\functions\\oee_timeseries' ensured to exist.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\functions\\functions.Function.yaml\n",
      "Successfully created 'functions.Function.yaml'.\n",
      "Creating file: C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\\modules\\bootcamp\\use_cases\\oee\\functions\\schedules.Schedule.yaml\n",
      "Successfully created 'schedules.Schedule.yaml'.\n",
      "\n",
      "All specified files and directories for OEE function have been processed.\n",
      "\n",
      "--- Important Post-Processing Steps for OEE Function ---\n",
      "1. Remember that the 'handler.py' file for 'oee_timeseries' function is expected to be in the 'oee_timeseries' directory (which has been created), but its content is not generated by this script.\n",
      "2. Ensure the authentication client IDs and secrets (e.g., `data_pipeline_oee_client_id`, `data_pipeline_oee_client_secret`) are correctly configured as environment variables in your deployment environment.\n",
      "3. If you need to pass specific 'sites' or 'lookback_minutes' to the OEE function beyond its defaults, you would add a 'data' section to the schedule in 'schedules.Schedule.yaml' (e.g., `data: { sites: [\"site1\"], lookback_minutes: 120 }`).\n"
     ]
    }
   ],
   "source": [
    "# This path should be the same as what you used for previous script executions.\n",
    "#my_base_path = r\"C:\\Users\\Prafull Ghare\\OneDrive - Cognite AS\\Documents\\codebase\\pune_bootcamp\\pune-rockwell-bootcamp\\ice-cream-dataops\"\n",
    "\n",
    "# Call the function to create the OEE function files\n",
    "create_oee_function_files(my_base_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
